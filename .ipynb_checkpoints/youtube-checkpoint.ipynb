{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f493dc2",
   "metadata": {},
   "source": [
    "# Corpus YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30461b31",
   "metadata": {},
   "source": [
    "## 1. Lectura de datos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc98573f-6644-4c5a-9a08-66dbbe12e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## abrimos los archivos\n",
    "\n",
    "import glob\n",
    "\n",
    "lista_files = glob.glob('youtube/*.txt')\n",
    "corpus = []\n",
    "\n",
    "for file in lista_files:\n",
    "    with open(file, 'r', encoding='utf8') as f:\n",
    "        corpus+=[f.read()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15522d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## número de textos\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04e0950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"Faceshopping\" by Sophie)\\n♪ My face is the front of shop ♪\\n♪ My face is the real shop front ♪\\n♪ My shop is the face I front ♪\\n♪ I\\'m real when I shop my face ♪\\n♪ Artificial bloom ♪\\n♪ Hydroponic skin ♪\\n♪ Chemical release ♪\\n♪ Synthesize the real ♪\\n(upb'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81bbb9",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd9dd9bf-b5ef-4505-8c67-ad3b187aea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## librerias\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e2fce56-63cb-495b-aaf8-fa7feae6b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jxver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20361ab2-6bda-46f4-b9bb-561d725c68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## función para remover () y []\n",
    "\n",
    "def a(test_str):\n",
    "    ret = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in test_str:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '(':\n",
    "            skip2c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == ')'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            ret += i\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14780019-9aa6-4c52-a49a-c526220df1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aplicamos la función \"a\" a cada texto del corpus\n",
    "\n",
    "corpus = [a(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0b97ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n♪ My face is the front of shop ♪\\n♪ My face is the real shop front ♪\\n♪ My shop is the face I front ♪\\n♪ I'm real when I shop my face ♪\\n♪ Artificial bloom ♪\\n♪ Hydroponic skin ♪\\n♪ Chemical release ♪\\n♪ Synthesize the real ♪\\n - Hey guys, it's Natalie,\\nwel\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6ae884b-c67b-4443-b710-6b0f085cb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dividimos por salto de linea cada texto\n",
    "\n",
    "corpus = [c.replace('♪','').split('\\n') for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34c624d8-96bd-449d-8378-ec716a91a987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' My face is the front of shop ',\n",
       " ' My face is the real shop front ',\n",
       " ' My shop is the face I front ',\n",
       " \" I'm real when I shop my face \"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac550f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eliminamos strings vacíos (cada texto)\n",
    "\n",
    "corpus = [[sentence.strip() for sentence in c if len(sentence)>0] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae6004d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## volvemos a juntar las oraciones :)\n",
    "\n",
    "corpus = [' '.join(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d62326-be95-4b3d-817f-a1bfbe6bc0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My face is the front of shop My face is the real shop front My shop is the face I front I'm real when I shop my face Artificial bloom Hydroponic skin Chemical release Synthesize the real - Hey guys, it's Natalie, welcome back to my channel. Today I'm\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34f16613-b9c2-4ec3-9a90-b9097145f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dividimos en oraciones cada texto\n",
    "\n",
    "sentence_list = []\n",
    "\n",
    "for texto in corpus:\n",
    "    sentences = sent_tokenize(texto)\n",
    "    sentence_list+=[sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "964d84c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba60933-2a54-4a8d-9400-618c95b1cd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My face is the front of shop My face is the real shop front My shop is the face I front I'm real when I shop my face Artificial bloom Hydroponic skin Chemical release Synthesize the real - Hey guys, it's Natalie, welcome back to my channel.\",\n",
       " \"Today I'm gonna do a makeup tutorial, as always, but first, story time, story time, story time!\",\n",
       " \"I just wanna be upfront with you guys and let you know that I've had some facial surgery.\",\n",
       " \"I'm always gonna be honest with you guys about this kind of thing because you mean so much to me.\",\n",
       " \"Like you've been here with me since the beginning, and you've seen my story, my whole entire journey, this journey I've been on as a transgender woman.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c86e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtramos por textos con al menos 2 oraciones\n",
    "\n",
    "sentence_list = [s for s in sentence_list if len(s)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cab91a6-580d-477e-9af2-f5a533a787f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero de oraciones por texto\n",
    "\n",
    "numero_oraciones = sum([len(s) for s in sentence_list])\n",
    "promedio_oraciones_por_texto = np.mean([len(s) for s in sentence_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fe5fdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7355, 319.7826086956522)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numero_oraciones, promedio_oraciones_por_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b513cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokens y types\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for texto in sentence_list:\n",
    "    for sentence in texto:\n",
    "        tokens+=sentence.split(' ')\n",
    "\n",
    "tokens = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee0ee92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142029, 17593)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## número de tokens y types en el corpus\n",
    "\n",
    "len(tokens),len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "686584ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## todos estos cálculos descriptivos deberían ir en una tabla :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85bc9c",
   "metadata": {},
   "source": [
    "## análisis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f2d03",
   "metadata": {},
   "source": [
    "### 1. Nominalizaciones\n",
    "El trabajo aquí es con listas de oraciones. Extraemos los sustantivos :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8870d8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\jxver\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (59.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (3.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jxver\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "248d1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "oraciones_nlp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4563f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## identificamos el lema y el pos\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for sentence_text in sentence_list:\n",
    "    if len(sentence_text)>1:\n",
    "        sent = []\n",
    "        for sentence in sentence_text:\n",
    "            doc = nlp(sentence)\n",
    "            sent += [[(token.lemma_.lower(),token.pos_) for token in doc]]\n",
    "        oraciones_nlp += [sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "131ef0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oraciones_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1c211ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_tokens_types(oraciones):\n",
    "    number = []\n",
    "    for oracion in oraciones:\n",
    "        number+=[pair[0] for pair in oracion]\n",
    "    return len(number),len(set(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31835204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6448, 1297)\n",
      "(20874, 2296)\n",
      "(16843, 2280)\n",
      "(2089, 422)\n",
      "(2998, 526)\n",
      "(2792, 572)\n",
      "(8217, 1491)\n",
      "(7271, 1246)\n",
      "(3523, 653)\n",
      "(6488, 1249)\n",
      "(5241, 1010)\n",
      "(8595, 1502)\n",
      "(8649, 1442)\n",
      "(8296, 1247)\n",
      "(7519, 1529)\n",
      "(10654, 1479)\n",
      "(6639, 1079)\n",
      "(9705, 1426)\n",
      "(3206, 794)\n",
      "(8006, 1025)\n",
      "(4441, 860)\n",
      "(4947, 920)\n",
      "(3566, 834)\n"
     ]
    }
   ],
   "source": [
    "## número de tokens y types para los 23 textos \n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    print(number_tokens_types(oraciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41bec5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_nouns(oraciones):\n",
    "    number = []\n",
    "    for oracion in oraciones:\n",
    "        number+=[pair for pair in oracion if pair[1]=='NOUN']\n",
    "    return len(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0661b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031\n",
      "2832\n",
      "2727\n",
      "320\n",
      "381\n",
      "383\n",
      "1282\n",
      "1077\n",
      "433\n",
      "956\n",
      "804\n",
      "1283\n",
      "1375\n",
      "1469\n",
      "1511\n",
      "1711\n",
      "1075\n",
      "1522\n",
      "508\n",
      "947\n",
      "627\n",
      "573\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "## número de nouns tokens y types para los 23 textos \n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    print(number_nouns(oraciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2bdafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## la pregunta es: ¿De este número de nouns cuántos son nominalizaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3894534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reglas de nominalization\n",
    "\n",
    "## reglas de nominalization\n",
    "## puse las palabras lematizadas, en ese caso, solo es necesario poner las palabras en singular\n",
    "\n",
    "no_nom = ['thing', 'things', 'somethings', 'something', 'anything', 'everything', 'nothing','original', 'special', 'normal', 'version', 'tutorial', 'moment']\n",
    "terminacion = ['ment','ments','tions', 'tion','sions', 'sion', 'ibility','ibilities', 'ity','ities', 'ness','nesses', 'al','als','ings', 'ing'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87126c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominalization(oraciones):\n",
    "    nom = []\n",
    "    for oracion in oraciones:\n",
    "        for pair in oracion:\n",
    "            if pair[0] not in no_nom:\n",
    "                if pair[1]=='NOUN':\n",
    "                    for END in terminacion:\n",
    "                        if pair[0].endswith(END):\n",
    "                            nom+=[pair[0]]\n",
    "    return nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "000c1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_list = []\n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    nom_list += [nominalization(oraciones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32401ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beginning',\n",
       " 'feminization',\n",
       " 'feminization',\n",
       " 'contouring',\n",
       " 'contouring',\n",
       " 'incision',\n",
       " 'fragment',\n",
       " 'incision',\n",
       " 'reconstruction',\n",
       " 'incision',\n",
       " 'anticipation',\n",
       " 'encouragement',\n",
       " 'jackal',\n",
       " 'vanity',\n",
       " 'feminization',\n",
       " 'reassignment',\n",
       " 'identity',\n",
       " 'argument',\n",
       " 'reality',\n",
       " 'ideal',\n",
       " 'transition',\n",
       " 'removal',\n",
       " 'grooming',\n",
       " 'thinking',\n",
       " 'realness',\n",
       " 'quality',\n",
       " 'painting',\n",
       " 'femininity',\n",
       " 'aspiration',\n",
       " 'aging',\n",
       " 'intensity',\n",
       " 'artificial',\n",
       " 'skinmaxing',\n",
       " 'business',\n",
       " 'discussion',\n",
       " 'masculinization',\n",
       " 'looksmaxing',\n",
       " 'deal',\n",
       " 'arousal',\n",
       " 'evolution',\n",
       " 'contouring',\n",
       " 'ingenuity',\n",
       " 'resourcefulness',\n",
       " 'individuality',\n",
       " 'femininity',\n",
       " 'exploitation',\n",
       " 'bartering',\n",
       " 'judgment',\n",
       " 'comment',\n",
       " 'comment',\n",
       " 'exaggeration',\n",
       " 'delusion',\n",
       " 'embellishment',\n",
       " 'darkness',\n",
       " 'invention',\n",
       " 'darkness',\n",
       " 'question',\n",
       " 'aging',\n",
       " 'ritual',\n",
       " 'cleansing',\n",
       " 'corporation',\n",
       " 'kindness',\n",
       " 'saving',\n",
       " 'collection',\n",
       " 'facial',\n",
       " 'futility',\n",
       " 'solution',\n",
       " 'spiral',\n",
       " 'obsession',\n",
       " 'advertising',\n",
       " 'relation',\n",
       " 'awareness',\n",
       " 'contemplation',\n",
       " 'revolution',\n",
       " 'revolution',\n",
       " 'revolution',\n",
       " 'futility',\n",
       " 'representation',\n",
       " 'celebrity',\n",
       " 'spiral',\n",
       " 'ideal',\n",
       " 'originality',\n",
       " 'appreciation',\n",
       " 'comment',\n",
       " 'superiority',\n",
       " 'solution',\n",
       " 'audacity',\n",
       " 'kindness']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ¿Cuántas de estas son realmente nominalizaciones? Por ejemplo, ¿qué ocurre con \"fragment\"? Eso podría mejorarse, o al menos\n",
    "## plantear formas en que podría mirarse de otras formas\n",
    "\n",
    "nom_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9b57f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nom_list = []\n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    num_nom_list += [len(nominalization(oraciones))/number_nouns(oraciones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c706d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08535402521823472,\n",
       " 0.1327683615819209,\n",
       " 0.12614594792812614,\n",
       " 0.13125,\n",
       " 0.09186351706036745,\n",
       " 0.0731070496083551,\n",
       " 0.11856474258970359,\n",
       " 0.08449396471680594,\n",
       " 0.08545034642032333,\n",
       " 0.12133891213389121,\n",
       " 0.10199004975124377,\n",
       " 0.10678098207326578,\n",
       " 0.104,\n",
       " 0.168141592920354,\n",
       " 0.171409662475182,\n",
       " 0.10461718293395675,\n",
       " 0.09023255813953489,\n",
       " 0.0873850197109067,\n",
       " 0.13385826771653545,\n",
       " 0.11404435058078141,\n",
       " 0.1371610845295056,\n",
       " 0.13787085514834205,\n",
       " 0.0899581589958159]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## porcentaje de nominalizaciones con respecto al total de tokens sustantivos para cada texto :)\n",
    "\n",
    "num_nom_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0419c8d-46bc-4a1f-aca5-044736826d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
